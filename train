#!/usr/bin/python3 
"""
Copyright 2021-2023 Salvatore Barone <salvatore.barone@unina.it>

This is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 3 of the License, or any later version.

This is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
more details.

You should have received a copy of the GNU General Public License along with
RMEncoder; if not, write to the Free Software Foundation, Inc., 51 Franklin
Street, Fifth Floor, Boston, MA 02110-1301, USA.
"""
import click, sys, os, json5, numpy as np
from src.dtgen import dtgen, training_with_parameter_tuning, save_dataset_to_csv
from src.git_updater import git_updater
from src.logger import configure_logger
from typing import Union

@click.group()
def cli():
    pass

@click.command("gensynth")
@click.argument('outputdir', type = click.Path(dir_okay=True, file_okay = False))
@click.option('-n', '--n_samples', type = int, required = True, help = "Number of samples")
@click.option('-f', '--n_features', type = int, required = True, help = "Number of features")
@click.option('-c', '--n_classes', type = int, required = True, help = "Number of classes")
@click.option('-w' , '--weights', type = str, default = None, help = "The proportions of samples assigned to each class. If None, then classes are balanced.")
@click.option('-v', '--verbose', type = click.Choice(["DEBUG", "INFO", "WARNINGS", "ERROR", "CRITICAL", 10, 20, 30, 40, 50]), default = "INFO")
def synthetic_dataset(outputdir, n_samples : int, n_features : int, n_classes : int, weights : str, verbose : Union[int,str]):
    """
    Generates a Synthetic dataset for classification problems
    
    OUTPUTDIR the output directory
    
    """
    configure_logger("pyALS-RF", verbose)
    from sklearn.datasets import make_classification
    from distutils.dir_util import mkpath
    if weights is not None:
        weights = [ float(w) for w in weights.replace("[", "").replace("]", "").replace(" ", "").split(",") ]
    x, y = make_classification(n_samples = n_samples, n_features = n_features, n_classes = n_classes, n_informative =  int(np.sqrt(n_features)), n_clusters_per_class = 1, weights = weights)
    attribute_names = [ f"feature_{i}" for i in range(n_features) ]
    mkpath(outputdir)
    save_dataset_to_csv(f"{outputdir}/dataset.csv", attribute_names, x, y)
    dataset_description = {
        "separator": ";",
        "outcome_col": -1,
        "skip_header": True,
        "attributes_name" : attribute_names,
        "classes_name" : [ f"{i}" for i in range(n_classes) ]
    }
    with open(f"{outputdir}/config.json5", "w") as f:
        json5.dump(dataset_description, f, indent = 2)
    print(f"Done generating synthetic dataset. Take a look at {outputdir}.")
    
    
@click.command("autotune")
@click.argument('clf', type = click.Choice(["dt", "rf", "wc"]), default = "rf")
@click.argument('tuning', type = click.Choice(["random", "grid"]), default = "random")
@click.argument('dataset', type=click.Path(exists=True, dir_okay=False))
@click.argument('configfile', type=click.Path(exists=True, dir_okay=False))
@click.argument('outputdir', type=click.Path(dir_okay=True), default = "output")
@click.option("-f", "--fraction", type = float, default = 0.9, help = "Fraction of data to be used as learning set. The remaining will be used as test data. In case the config.json enables different train and test datasets this option is not considered.")
#"Instead of using the entire training set, this function uses a small portion of the dataset, already splitted, for configuration. This option is enabled in the config.json of the dataset."
@click.option("-h",'--hyp_dataset', type=click.Path( dir_okay=False))
# "Separate testing dataset from dataset. If specified in the config.json of the dataset, this variable specifies the path of a separate testing dataset."
@click.option("-t",'--test_dataset',  type=click.Path(dir_okay=False))
@click.option("-n", "--ntrees", type = int, default = 1, help = "The number of trees in the ensemble. Ignored if clf is dt.")
@click.option("-i", "--niter", type = int, default = 150, help = "The number of iterations")
@click.option("-x", "--crossvalidate", is_flag = True, help = "Enable crossvalidation between sklearn and pyALS-RF")
@click.option("-j", "--ncpus", type = int, default = -1, help = "Number of cpus used.")
@click.option('-v', '--verbose', type = click.Choice(["DEBUG", "INFO", "WARNINGS", "ERROR", "CRITICAL", 10, 20, 30, 40, 50]), default = "INFO")
def parameter_tuning(clf, tuning, dataset, configfile, outputdir, fraction, hyp_dataset, test_dataset, ntrees, niter, crossvalidate, ncpus, verbose : Union[int,str]):
    """
    Generates a decision tree / random forest from CSV data using random search.
    
    TUNING is the hyperparameter tuning strategy (random/grid based)

    CLF is the classifier type: "dt" stands for decision tree, "rf" for random forest.

    DATASET is the csv file to be used as a learning set.

    CONFIGFILE configuration file providing further information to the tool
    
    OUTPUTDIR is the path of the output directory.
    """
    logger = configure_logger("pyALS-RF", verbose)
    logger.info("Performing learning with automatic hyperparamiter tuning"
                f"\n\tclf: {clf}"
                f"\n\ttuning: {tuning}"
                f"\n\tdataset: {dataset}"
                f"\n\thyp_dataset: {hyp_dataset}"
                f"\n\ttest_dataset: {test_dataset}"
                f"\n\tconfigfile: {configfile}"
                f"\n\toutputdir: {outputdir}"
                f"\n\tfraction: {fraction}"
                f"\n\n_cpus: {ncpus}"
                )
    training_with_parameter_tuning(clf, tuning, dataset, configfile, outputdir, fraction, ntrees, niter, crossvalidate, hyp_dataset, test_dataset, ncpus)
    
@click.command("dtgen")
@click.argument('clf', type = click.Choice(["dt", "rf"]), default = "rf")
@click.argument('dataset', type=click.Path(exists=True, dir_okay=False))
@click.argument('configfile', type=click.Path(exists=True, dir_okay=False))
@click.argument('outputdir', type=click.Path(dir_okay=True), default = "output")
@click.option("-f", "--fraction", type = float, default = 0.9, help = "Fraction of data to be used as learning set. The remaining will be used as test data.")
@click.option("-d", "--depth", type = int, default = None, help = "Maximum depth of the tree(s)")
@click.option("-n", "--ntrees", type = int, default = 1, help = "The number of trees in the ensemble. Ignored if clf is dt.")
@click.option("--criterion", type = click.Choice(["gini", "entropy", "log_loss"]), default = "gini", help = "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain")
@click.option("--min_samples_split", type = int, default = 2, help = "The minimum number of samples required to split an internal node.")
@click.option("--min_samples_leaf", type = int, default = 1, help = "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches.")
@click.option("--max_features", default = None, help = "The number of features to consider when looking for the best split. You can also specify one between \"sqrt\", \"log2\" or \"auto\"")
@click.option("--max_leaf_nodes", type = int, default = None, help = "Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.")
@click.option("--min_impurity_decrease", type = float, default = 0.0, help = "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.")
@click.option("--ccp_alpha", type = float, default = 0.0, help = "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed.")
@click.option("-b", "--disable_bootstrap", is_flag = True, help = "Disable the bootstrap")
@click.option("-x", "--crossvalidate", is_flag = True, help = "Enable crossvalidation between sklearn and pyALS-RF")
@click.option('-v', '--verbose', type = click.Choice(["DEBUG", "INFO", "WARNINGS", "ERROR", "CRITICAL", 10, 20, 30, 40, 50]), default = "INFO")
def learning(clf : str, dataset : str, configfile : str, outputdir : str, fraction : float, depth : int, ntrees : int, criterion : str, min_samples_split : int, min_samples_leaf : int, max_features : Union[int,str], max_leaf_nodes : int, min_impurity_decrease : float, ccp_alpha : float, disable_bootstrap : bool, crossvalidate : bool, verbose : Union[int,str]):
    """
    Generates a decision tree/ random forest from CSV data.

    CLF is the classifier type: "dt" stands for decision tree, "rf" for random forest.

    DATASET is the csv file to be used as a learning set.

    CONFIGFILE configuration file providing further information to the tool
    
    OUTPUTDIR is the path of the output directory.
    """
    configure_logger("pyALS-RF", verbose)
    dtgen(clf, dataset, configfile, outputdir, fraction, depth, ntrees, criterion, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, min_impurity_decrease, ccp_alpha, disable_bootstrap, crossvalidate)

cli.add_command(synthetic_dataset)
cli.add_command(parameter_tuning)
cli.add_command(learning)

if __name__ == '__main__':
    if git_updater(os.path.dirname(os.path.realpath(__file__))):
        os.execv(sys.argv[0], sys.argv)
    else:
        cli()